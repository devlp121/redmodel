{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bottom-bidder",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "import gzip as gzip_lib\n",
    "import numpy as np\n",
    "import os\n",
    "import pprint\n",
    "import shutil\n",
    "import tempfile\n",
    "import urllib\n",
    "import uuid\n",
    "import datetime\n",
    "import pandas as pd\n",
    "pp = pprint.PrettyPrinter()\n",
    "\n",
    "import tensorflow as tf\n",
    "import neural_structured_learning as nsl\n",
    "\n",
    "import tfx\n",
    "from tfx.components.evaluator.component import Evaluator\n",
    "from tfx.components import CsvExampleGen\n",
    "from tfx.components.example_gen.import_example_gen.component import ImportExampleGen\n",
    "from tfx.components.example_validator.component import ExampleValidator\n",
    "from tfx.components.model_validator.component import ModelValidator\n",
    "from tfx.components.pusher.component import Pusher\n",
    "from tfx.components.schema_gen.component import SchemaGen\n",
    "from tfx.components.statistics_gen.component import StatisticsGen\n",
    "from tfx.components.trainer.component import Trainer\n",
    "from tfx.components.transform.component import Transform\n",
    "from tfx.orchestration.experimental.interactive.interactive_context import InteractiveContext\n",
    "from tfx.proto import evaluator_pb2\n",
    "from tfx.proto import example_gen_pb2\n",
    "from tfx.proto import pusher_pb2\n",
    "from tfx.proto import trainer_pb2\n",
    "from tfx.utils.dsl_utils import external_input\n",
    "\n",
    "from tfx.types import artifact\n",
    "from tfx.types import artifact_utils\n",
    "from tfx.types import channel\n",
    "from tfx.types import standard_artifacts\n",
    "from tfx.types.standard_artifacts import Examples\n",
    "\n",
    "from tfx.dsl.component.experimental.annotations import InputArtifact\n",
    "from tfx.dsl.component.experimental.annotations import OutputArtifact\n",
    "from tfx.dsl.component.experimental.annotations import Parameter\n",
    "from tfx.dsl.component.experimental.decorators import component\n",
    "\n",
    "from tensorflow_metadata.proto.v0 import anomalies_pb2\n",
    "from tensorflow_metadata.proto.v0 import schema_pb2\n",
    "from tensorflow_metadata.proto.v0 import statistics_pb2\n",
    "\n",
    "import tensorflow_data_validation as tfdv\n",
    "import tensorflow_transform as tft\n",
    "import tensorflow_model_analysis as tfma\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "print(\"TF Version: \", tf.__version__)\n",
    "print(\"Eager mode: \", tf.executing_eagerly())\n",
    "print(\n",
    "    \"GPU is\",\n",
    "    \"available\" if tf.config.list_physical_devices(\"GPU\") else \"NOT AVAILABLE\")\n",
    "print(\"NSL Version: \", nsl.__version__)\n",
    "print(\"TFDV version: \", tfdv.__version__)\n",
    "print(\"TFT version: \", tft.__version__)\n",
    "print(\"TFMA version: \", tfma.__version__)\n",
    "print(\"Hub version: \", hub.__version__)\n",
    "print(\"Beam version: \", beam.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dutch-relations",
   "metadata": {},
   "outputs": [],
   "source": [
    "_pipeline_name = 'redmodel'\n",
    "\n",
    "_mh_root = os.path.join(os.environ['HOME'], 'redmodel')\n",
    "_data_root = os.path.join(_mh_root, 'data', 'simple')\n",
    "# Python module file to inject customized logic into the TFX components. The\n",
    "# Transform and Trainer both require user-defined functions to run successfully.\n",
    "_airflow_dir = os.path.join(os.environ['HOME'], 'airflow')\n",
    "\n",
    "_module_root = os.path.join(_airflow_dir, 'utils')\n",
    "_transform_module_file = os.path.join(_module_root,'unsup', 'transform.py')\n",
    "_trainer_module_file = os.path.join(_module_root,'unsup', 'trainer.py')# Path which can be listened to by the model server.  Pusher will output the\n",
    "# trained model here.\n",
    "_serving_model_dir = os.path.join(_mh_root, 'serving_model', _pipeline_name)\n",
    "\n",
    "# Directory and data locations.  This example assumes all of the chicago taxi\n",
    "# example code and metadata library is relative to $HOME, but you can store\n",
    "# these files anywhere on your local filesystem.\n",
    "_tfx_root = os.path.join(os.environ['HOME'], 'tfx')\n",
    "_pipeline_root = os.path.join(_mh_root, 'pipelines', _pipeline_name)\n",
    "# Sqlite ML-metadata db path.\n",
    "_metadata_path = os.path.join(_mh_root, 'metadata', _pipeline_name,\n",
    "                              'metadata.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disturbed-continent",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Airflow-specific configs; these will be passed directly to airflow\n",
    "_airflow_config = {\n",
    "    'schedule_interval': None,\n",
    "    'start_date': datetime.datetime(2020, 1, 1),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lucky-sleeping",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_csv_dataset(path):\n",
    "    csv_data = pd.read_csv(path)\n",
    "    return csv_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adult-prairie",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = _load_csv_dataset(_data_root+\"/train/train.csv\").values\n",
    "unsup_set = _load_csv_dataset(_data_root+\"/unsup/unsup.csv\")[:1000].values\n",
    "eval_set = _load_csv_dataset(_data_root+\"/eval/test.csv\").values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-deputy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following functions can be used to convert a value to a type compatible\n",
    "# with tf.train.Example.\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    \n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value.tobytes()]))\n",
    "\n",
    "def _int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entire-screw",
   "metadata": {},
   "outputs": [],
   "source": [
    "def serialize_example(label, text):\n",
    "  # Create a dictionary mapping the feature name to the tf.train.Example-compatible\n",
    "  # data type for supervised examples.\n",
    "    feature = {\n",
    "      'label': _int64_feature(label),\n",
    "      'text': _bytes_feature(text),\n",
    "      }\n",
    "  # Create a Features message using tf.train.Example.\n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    return example_proto.SerializeToString()\n",
    "\n",
    "\n",
    "def serialize_example_unsup(text):\n",
    "  # Create a dictionary mapping the feature name to the tf.train.Example-compatible\n",
    "  # data type for supervised examples.\n",
    "    feature = {\n",
    "      'text': _bytes_feature(text),\n",
    "      }\n",
    "  # Create a Features message using tf.train.Example.\n",
    "    example_proto = tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "    return example_proto.SerializeToString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specified-humor",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_path = tempfile.mkdtemp(prefix=\"tfx-data\")\n",
    "train_path = os.path.join(examples_path, \"train.tfrecord\")\n",
    "eval_path = os.path.join(examples_path, \"eval.tfrecord\")\n",
    "unsup_path = os.path.join(examples_path, \"unsup.tfrecord\")\n",
    "train_temp_path=os.path.join(examples_path, \"train_temp.tfrecord\")\n",
    "\n",
    "\n",
    "# The following functions can be used to convert a value to a type compatible\n",
    "# with tf.train.Example.\n",
    "\n",
    "for path, dataset in [(train_path, train_set), (eval_path, eval_set)]:\n",
    "    with tf.io.TFRecordWriter(path) as writer:\n",
    "        for example in dataset:\n",
    "            features, label = example[:-1], example[-1]\n",
    "            writer.write(\n",
    "                serialize_example(\n",
    "                    label=label, text=features\n",
    "                ))\n",
    "\n",
    "for path, dataset in [(unsup_path, unsup_set)]:\n",
    "    with tf.io.TFRecordWriter(path) as writer:\n",
    "        for example in dataset:\n",
    "            feature = example[1]\n",
    "            writer.write(\n",
    "                serialize_example_unsup(\n",
    "                    text=features\n",
    "                ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nuclear-reference",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "typical-three",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "muslim-vitamin",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artificial-tribune",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = InteractiveContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "published-resolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_config = example_gen_pb2.Input(splits=[\n",
    "        example_gen_pb2.Input.Split(name='train', pattern='train.tfrecord'),\n",
    "        example_gen_pb2.Input.Split(name='eval', pattern='eval.tfrecord')\n",
    "    ])\n",
    "    \n",
    "example_gen = ImportExampleGen(input_base=examples_path, input_config=input_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loving-aruba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinguished-richardson",
   "metadata": {},
   "outputs": [],
   "source": [
    "context.run(example_gen, enable_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acoustic-atlanta",
   "metadata": {},
   "outputs": [],
   "source": [
    "for artifact in example_gen.outputs['examples'].get():\n",
    "    print(artifact)\n",
    "\n",
    "print('\\nexample_gen.outputs is a {}'.format(type(example_gen.outputs)))\n",
    "print(example_gen.outputs)\n",
    "\n",
    "print(example_gen.outputs['examples'].get()[0].split_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promising-soviet",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_example_with_unique_id(example, id_feature_name):\n",
    "    \"\"\"Adds a unique ID to the given `tf.train.Example` proto.\n",
    "\n",
    "    This function uses Python's 'uuid' module to generate a universally unique\n",
    "    identifier for each example.\n",
    "\n",
    "    Args:\n",
    "    example: An instance of a `tf.train.Example` proto.\n",
    "    id_feature_name: The name of the feature in the resulting `tf.train.Example`\n",
    "      that will contain the unique identifier.\n",
    "\n",
    "    Returns:\n",
    "    A new `tf.train.Example` proto that includes a unique identifier as an\n",
    "    additional feature.\n",
    "    \"\"\"\n",
    "    \n",
    "  \n",
    "    result = tf.train.Example()\n",
    "    result.CopyFrom(example)\n",
    "    unique_id = uuid.uuid4()\n",
    "    result.features.feature.get_or_create(\n",
    "        id_feature_name).bytes_list.MergeFrom(\n",
    "            tf.train.BytesList(value=[str(unique_id).encode('utf-8')]))\n",
    "    return result\n",
    "\n",
    "\n",
    "@component\n",
    "def IdentifyExamples(orig_examples: InputArtifact[Examples],\n",
    "                     identified_examples: OutputArtifact[Examples],\n",
    "                     id_feature_name: Parameter[str],\n",
    "                     component_name: Parameter[str]) -> None:\n",
    "\n",
    "  # Get a list of the splits in input_data\n",
    "    splits_list = artifact_utils.decode_split_names(\n",
    "        split_names=orig_examples.split_names)\n",
    "\n",
    "    for split in splits_list:\n",
    "        input_dir = os.path.join(orig_examples.uri, split)\n",
    "        output_dir = os.path.join(identified_examples.uri, split)\n",
    "        os.mkdir(output_dir)\n",
    "        with beam.Pipeline() as pipeline:\n",
    "          (pipeline\n",
    "           | 'ReadExamples' >> beam.io.ReadFromTFRecord(\n",
    "               os.path.join(input_dir, '*'),\n",
    "               coder=beam.coders.coders.ProtoCoder(tf.train.Example))\n",
    "           | 'AddUniqueId' >> beam.Map(make_example_with_unique_id, id_feature_name)\n",
    "           | 'WriteIdentifiedExamples' >> beam.io.WriteToTFRecord(\n",
    "               file_path_prefix=os.path.join(output_dir, 'data_tfrecord'),\n",
    "               coder=beam.coders.coders.ProtoCoder(tf.train.Example),\n",
    "               file_name_suffix='.gz'))\n",
    "\n",
    "    # For completeness, encode the splits names and payload_format.\n",
    "    # We could also just use input_data.split_names.\n",
    "    identified_examples.split_names = artifact_utils.encode_split_names(\n",
    "        splits=splits_list)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternate-product",
   "metadata": {},
   "outputs": [],
   "source": [
    "identify_examples = IdentifyExamples(\n",
    "    orig_examples=example_gen.outputs['examples'],\n",
    "    component_name=u'IdentifyExamples',\n",
    "    id_feature_name=u'id')\n",
    "context.run(identify_examples, enable_cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "duplicate-statistics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes statistics over data for visualization and example validation.\n",
    "statistics_gen = StatisticsGen(\n",
    "    examples=identify_examples.outputs[\"identified_examples\"])\n",
    "context.run(statistics_gen, enable_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vietnamese-housing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates schema based on statistics files.\n",
    "schema_gen = SchemaGen(statistics=statistics_gen.outputs['statistics'])\n",
    "context.run(schema_gen, enable_cache=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italian-armenia",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_uri = schema_gen.outputs['schema'].get()[0].uri\n",
    "schema_filename = os.path.join(train_uri, 'schema.pbtxt')\n",
    "schema = tfx.utils.io_utils.parse_pbtxt_file(\n",
    "    file_name=schema_filename, message=schema_pb2.Schema())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exotic-manner",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfdv.display_schema(schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strong-turning",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs anomaly detection based on statistics and data schema.\n",
    "validate_stats = ExampleValidator(\n",
    "    statistics=statistics_gen.outputs['statistics'],\n",
    "    schema=schema_gen.outputs['schema'])\n",
    "context.run(validate_stats, enable_cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fiscal-promotion",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_airflow_root = os.path.join(os.environ['HOME'], 'airflow')\n",
    "\n",
    "\n",
    "_transform_module_file = os.path.join(_airflow_root,'utils', 'transform.py')\n",
    "_trainer_module_file = os.path.join(_airflow_root,'utils', 'trainer.py')\n",
    "_module_file = os.path.join(_airflow_root,'utils', 'module.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functioning-sunglasses",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experienced-butler",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silver-pierce",
   "metadata": {},
   "outputs": [],
   "source": [
    "swivel_url = 'https://tfhub.dev/google/tf2-preview/gnews-swivel-20dim/1'\n",
    "hub_layer = hub.KerasLayer(swivel_url, input_shape=[], dtype=tf.string)\n",
    "\n",
    "\n",
    "def _bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\n",
    "\n",
    "\n",
    "def _float_feature(value):\n",
    "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=value))\n",
    "\n",
    "\n",
    "def create_embedding_example(example):\n",
    "    \"\"\"Create tf.Example containing the sample's embedding and its ID.\"\"\"\n",
    "    sentence_embedding = hub_layer(tf.sparse.to_dense(example['text']))\n",
    "    \n",
    "    \n",
    "    # Flatten the sentence embedding back to 1-D.\n",
    "    sentence_embedding = tf.reshape(sentence_embedding, shape=[-1])\n",
    "    feature_dict = {\n",
    "        'id': _bytes_feature(tf.sparse.to_dense(example['id']).numpy()),\n",
    "        'embedding': _float_feature(sentence_embedding.numpy().tolist())\n",
    "    }\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature_dict))\n",
    "\n",
    "\n",
    "def create_dataset(uri):\n",
    "    tfrecord_filenames = [os.path.join(uri, name) for name in os.listdir(uri)]\n",
    "    return tf.data.TFRecordDataset(tfrecord_filenames, compression_type='GZIP')\n",
    "\n",
    "\n",
    "def create_embeddings(train_path, output_path):\n",
    "    dataset = create_dataset(train_path)\n",
    "    embeddings_path = os.path.join(output_path, 'embeddings.tfr')\n",
    "    \n",
    "    feature_map = {\n",
    "        'label': tf.io.FixedLenFeature([], tf.int64),\n",
    "        'id': tf.io.VarLenFeature(tf.string),\n",
    "        'text': tf.io.VarLenFeature(tf.string)\n",
    "    }\n",
    "\n",
    "    with tf.io.TFRecordWriter(embeddings_path) as writer:\n",
    "        for tfrecord in dataset:\n",
    "            tensor_dict = tf.io.parse_single_example(tfrecord, feature_map)\n",
    "            embedding_example = create_embedding_example(tensor_dict)\n",
    "            writer.write(embedding_example.SerializeToString())\n",
    "\n",
    "\n",
    "def build_graph(output_path, similarity_threshold):\n",
    "    embeddings_path = os.path.join(output_path, 'embeddings.tfr')\n",
    "    graph_path = os.path.join(output_path, 'graph.tfv')\n",
    "    nsl.tools.build_graph([embeddings_path], graph_path, similarity_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animal-halloween",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Custom Artifact type\"\"\"\n",
    "\n",
    "\n",
    "class SynthesizedGraph(tfx.types.artifact.Artifact):\n",
    "    \"\"\"Output artifact of the SynthesizeGraph component\"\"\"\n",
    "    TYPE_NAME = 'SynthesizedGraphPath'\n",
    "    PROPERTIES = {\n",
    "        'span': standard_artifacts.SPAN_PROPERTY,\n",
    "        'split_names': standard_artifacts.SPLIT_NAMES_PROPERTY,\n",
    "    }\n",
    "\n",
    "\n",
    "@component\n",
    "def SynthesizeGraph(identified_examples: InputArtifact[Examples],\n",
    "                    synthesized_graph: OutputArtifact[SynthesizedGraph],\n",
    "                    similarity_threshold: Parameter[float],\n",
    "                    component_name: Parameter[str]) -> None:\n",
    "    # Get a list of the splits in input_data\n",
    "    splits_list = artifact_utils.decode_split_names(\n",
    "    split_names=identified_examples.split_names)\n",
    "    \n",
    "    \n",
    "    # We build a graph only based on the 'train' split which includes both\n",
    "    # labeled and unlabeled examples.\n",
    "    train_input_examples_uri = os.path.join(identified_examples.uri, 'train')\n",
    "    output_graph_uri = os.path.join(synthesized_graph.uri, 'train')\n",
    "    os.mkdir(output_graph_uri)\n",
    "    \n",
    "    \n",
    "    print('Creating embeddings...')\n",
    "    create_embeddings(train_input_examples_uri, output_graph_uri)\n",
    "    \n",
    "    print('Synthesizing graph...')\n",
    "    build_graph(output_graph_uri, similarity_threshold)\n",
    "    \n",
    "    synthesized_graph.split_names = artifact_utils.encode_split_names(\n",
    "      splits=['train'])\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developed-savannah",
   "metadata": {},
   "outputs": [],
   "source": [
    "synthesize_graph = SynthesizeGraph(\n",
    "    identified_examples=identify_examples.outputs['identified_examples'],\n",
    "    component_name=u'SynthesizeGraph',\n",
    "    similarity_threshold=0.99)\n",
    "context.run(synthesize_graph, enable_cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greenhouse-western",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performs transformations and feature engineering in training and serving.\n",
    "transform = Transform(\n",
    "    examples=identify_examples.outputs['identified_examples'],\n",
    "    schema=schema_gen.outputs['schema'],\n",
    "    module_file=_transform_module_file)\n",
    "context.run(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "religious-reunion",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_uri = transform.outputs['transform_graph'].get()[0].uri\n",
    "os.listdir(train_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "administrative-company",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pprint_examples(artifact, n_examples=3):\n",
    "  print(\"artifact:\", artifact)\n",
    "  uri = os.path.join(artifact.uri, \"train\")\n",
    "  print(\"uri:\", uri)\n",
    "  tfrecord_filenames = [os.path.join(uri, name) for name in os.listdir(uri)]\n",
    "  print(\"tfrecord_filenames:\", tfrecord_filenames)\n",
    "  dataset = tf.data.TFRecordDataset(tfrecord_filenames, compression_type=\"GZIP\")\n",
    "  for tfrecord in dataset.take(n_examples):\n",
    "    serialized_example = tfrecord.numpy()\n",
    "    example = tf.train.Example.FromString(serialized_example)\n",
    "    pp.pprint(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alpine-pollution",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint_examples(transform.outputs['transformed_examples'].get()[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attended-standing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_and_unsup(input_uri):\n",
    "  'Separate the labeled and unlabeled instances.'\n",
    "\n",
    "  tmp_dir = tempfile.mkdtemp(prefix='tfx-data')\n",
    "  tfrecord_filenames = [\n",
    "      os.path.join(input_uri, filename) for filename in os.listdir(input_uri)\n",
    "  ]\n",
    "  train_path = os.path.join(tmp_dir, 'train.tfrecord')\n",
    "  unsup_path = os.path.join(tmp_dir, 'unsup.tfrecord')\n",
    "  with tf.io.TFRecordWriter(train_path) as train_writer, \\\n",
    "       tf.io.TFRecordWriter(unsup_path) as unsup_writer:\n",
    "    for tfrecord in tf.data.TFRecordDataset(\n",
    "        tfrecord_filenames, compression_type='GZIP'):\n",
    "      example = tf.train.Example()\n",
    "      example.ParseFromString(tfrecord.numpy())\n",
    "      if ('label_xf' not in example.features.feature or\n",
    "          example.features.feature['label_xf'].int64_list.value[0] == -1):\n",
    "        writer = unsup_writer\n",
    "      else:\n",
    "        writer = train_writer\n",
    "      writer.write(tfrecord.numpy())\n",
    "  return train_path, unsup_path\n",
    "\n",
    "\n",
    "def gzip(filepath):\n",
    "  with open(filepath, 'rb') as f_in:\n",
    "    with gzip_lib.open(filepath + '.gz', 'wb') as f_out:\n",
    "      shutil.copyfileobj(f_in, f_out)\n",
    "  os.remove(filepath)\n",
    "\n",
    "\n",
    "def copy_tfrecords(input_uri, output_uri):\n",
    "  for filename in os.listdir(input_uri):\n",
    "    input_filename = os.path.join(input_uri, filename)\n",
    "    output_filename = os.path.join(output_uri, filename)\n",
    "    shutil.copyfile(input_filename, output_filename)\n",
    "\n",
    "\n",
    "@component\n",
    "def GraphAugmentation(identified_examples: InputArtifact[Examples],\n",
    "                      synthesized_graph: InputArtifact[SynthesizedGraph],\n",
    "                      augmented_examples: OutputArtifact[Examples],\n",
    "                      num_neighbors: Parameter[int],\n",
    "                      component_name: Parameter[str]) -> None:\n",
    "\n",
    "  # Get a list of the splits in input_data\n",
    "  splits_list = artifact_utils.decode_split_names(\n",
    "      split_names=identified_examples.split_names)\n",
    "\n",
    "  train_input_uri = os.path.join(identified_examples.uri, 'train')\n",
    "  eval_input_uri = os.path.join(identified_examples.uri, 'eval')\n",
    "  train_graph_uri = os.path.join(synthesized_graph.uri, 'train')\n",
    "  train_output_uri = os.path.join(augmented_examples.uri, 'train')\n",
    "  eval_output_uri = os.path.join(augmented_examples.uri, 'eval')\n",
    "\n",
    "  os.mkdir(train_output_uri)\n",
    "  os.mkdir(eval_output_uri)\n",
    "\n",
    "  # Separate out the labeled and unlabeled examples from the 'train' split.\n",
    "  train_path, unsup_path = split_train_and_unsup(train_input_uri)\n",
    "\n",
    "  output_path = os.path.join(train_output_uri, 'nsl_train_data.tfr')\n",
    "  pack_nbrs_args = dict(\n",
    "      labeled_examples_path=train_path,\n",
    "      unlabeled_examples_path=unsup_path,\n",
    "      graph_path=os.path.join(train_graph_uri, 'graph.tfv'),\n",
    "      output_training_data_path=output_path,\n",
    "      add_undirected_edges=True,\n",
    "      max_nbrs=num_neighbors)\n",
    "  print('nsl.tools.pack_nbrs arguments:', pack_nbrs_args)\n",
    "  nsl.tools.pack_nbrs(**pack_nbrs_args)\n",
    "\n",
    "  # Downstream components expect gzip'ed TFRecords.\n",
    "  gzip(output_path)\n",
    "\n",
    "  # The test examples are left untouched and are simply copied over.\n",
    "  copy_tfrecords(eval_input_uri, eval_output_uri)\n",
    "\n",
    "  augmented_examples.split_names = identified_examples.split_names\n",
    "\n",
    "  return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "headed-blake",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augments training data with graph neighbors.\n",
    "graph_augmentation = GraphAugmentation(\n",
    "    identified_examples=transform.outputs['transformed_examples'],\n",
    "    synthesized_graph=synthesize_graph.outputs['synthesized_graph'],\n",
    "    component_name=u'GraphAugmentation',\n",
    "    num_neighbors=3)\n",
    "context.run(graph_augmentation, enable_cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animated-species",
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint_examples(graph_augmentation.outputs['augmented_examples'].get()[0], 6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "likely-crowd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    module_file=_trainer_module_file,\n",
    "    transformed_examples=graph_augmentation.outputs['augmented_examples'],\n",
    "    schema=schema_gen.outputs['schema'],\n",
    "    transform_graph=transform.outputs['transform_graph'],\n",
    "    train_args=trainer_pb2.TrainArgs(num_steps=10000),\n",
    "    eval_args=trainer_pb2.EvalArgs(num_steps=5000))\n",
    "context.run(trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automated-oregon",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entitled-vietnam",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "active-disco",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gorgeous-profit",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "critical-musician",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functional-velvet",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "damaged-discipline",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "together-founder",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "critical-color",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electoral-thesaurus",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "improving-village",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "studied-account",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "miniature-terrace",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liable-humor",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
